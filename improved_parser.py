import requests
from bs4 import BeautifulSoup
import pandas as pd
from time import sleep
import random
import json
import logging
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin, urlparse
import re
from typing import List, Dict, Optional

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OLXParserDebug:
    def __init__(self):
        self.base_url = "https://www.olx.kz"
        self.search_url = "https://www.olx.kz/nedvizhimost/kommercheskie-pomeshcheniya/arenda/"
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        self.session = requests.Session()
        self.max_pages = 3  # Ð£Ð¼ÐµÐ½ÑŒÑˆÐ¸Ð»Ð¸ Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸
        self.request_timeout = 10
        self.output_file = "olx_commercial_debug.csv"
        
    def setup_session(self) -> None:
        """ÐÐ°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°ÐµÑ‚ ÑÐµÑÑÐ¸ÑŽ Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°Ð¼Ð¸"""
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })

    def scrape_page_debug(self, page_url: str) -> List[Dict]:
        """ÐžÑ‚Ð»Ð°Ð´Ð¾Ñ‡Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹"""
        try:
            self.session.headers['User-Agent'] = random.choice(self.user_agents)
            response = self.session.get(page_url, timeout=self.request_timeout)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ð˜Ñ‰ÐµÐ¼ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ
            items = soup.select('div[data-cy="l-card"]')
            logger.info(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(items)} ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² div[data-cy='l-card']")
            
            if not items:
                # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð¸ Ð²Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ
                logger.warning("ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€ Ð½Ðµ ÑÑ€Ð°Ð±Ð¾Ñ‚Ð°Ð», Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ...")
                self._debug_page_structure(soup)
                return []
            
            listings = []
            
            for i, item in enumerate(items[:5]):  # Ð‘ÐµÑ€ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 5 Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸
                logger.info(f"\n--- ÐžÑ‚Ð»Ð°Ð´ÐºÐ° Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ #{i+1} ---")
                listing_data = self._extract_listing_data_debug(item)
                if listing_data:
                    listings.append(listing_data)
                    
            return listings
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ðµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹: {e}")
            return []

    def _debug_page_structure(self, soup: BeautifulSoup):
        """ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸"""
        logger.info("=== ÐÐÐÐ›Ð˜Ð— Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð« Ð¡Ð¢Ð ÐÐÐ˜Ð¦Ð« ===")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ðµ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹
        selectors_to_check = [
            'div[data-cy="l-card"]',
            '.css-1sw7q4x',
            '[data-testid="l-card"]',
            '.offer-wrapper',
            'div[data-marker="item"]',
            'article',
            '.ad-card'
        ]
        
        for selector in selectors_to_check:
            elements = soup.select(selector)
            logger.info(f"Ð¡ÐµÐ»ÐµÐºÑ‚Ð¾Ñ€ '{selector}': Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ {len(elements)} ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
            
            if elements and len(elements) > 0:
                # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°
                first_elem = elements[0]
                logger.info(f"ÐŸÐµÑ€Ð²Ñ‹Ð¹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚ '{selector}':")
                logger.info(f"  - Ð¢ÐµÐ³: {first_elem.name}")
                logger.info(f"  - ÐšÐ»Ð°ÑÑÑ‹: {first_elem.get('class', [])}")
                logger.info(f"  - ÐÑ‚Ñ€Ð¸Ð±ÑƒÑ‚Ñ‹: {list(first_elem.attrs.keys())}")
                
                # Ð˜Ñ‰ÐµÐ¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸ Ð²Ð½ÑƒÑ‚Ñ€Ð¸
                titles = first_elem.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                logger.info(f"  - ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¾Ð²: {len(titles)}")
                for title in titles[:2]:
                    logger.info(f"    â€¢ {title.name}: '{title.get_text(strip=True)[:50]}...'")

    def _extract_listing_data_debug(self, item) -> Optional[Dict]:
        """ÐžÑ‚Ð»Ð°Ð´Ð¾Ñ‡Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        logger.info(f"Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°: {item.name}, ÐºÐ»Ð°ÑÑÑ‹: {item.get('class', [])}")
        
        # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð½Ñ‹Ðµ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð´Ð»Ñ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°
        title_selectors = [
            'h6', 'h4', 'h3', 'h2', 'h1',
            '[data-cy="ad-card-title"]', 
            'a[data-cy="listing-ad-title"]',
            '.css-16v5mdi',
            '.css-u2ayx9'
        ]
        
        title = "N/A"
        for selector in title_selectors:
            title_elem = item.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                logger.info(f"âœ… Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº Ð½Ð°Ð¹Ð´ÐµÐ½ Ñ‡ÐµÑ€ÐµÐ· '{selector}': '{title[:50]}...'")
                break
            else:
                logger.debug(f"âŒ Ð¡ÐµÐ»ÐµÐºÑ‚Ð¾Ñ€ '{selector}' Ð½Ðµ ÑÑ€Ð°Ð±Ð¾Ñ‚Ð°Ð»")
        
        # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð½Ñ‹Ðµ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð´Ð»Ñ Ñ†ÐµÐ½Ñ‹
        price_selectors = [
            '[data-testid="ad-price"]',
            '.price',
            '[data-cy="ad-card-price"]',
            '.css-10b0gli',
            '.css-1uwte2c'
        ]
        
        price_text = "N/A"
        for selector in price_selectors:
            price_elem = item.select_one(selector)
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                logger.info(f"âœ… Ð¦ÐµÐ½Ð° Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ñ‡ÐµÑ€ÐµÐ· '{selector}': '{price_text}'")
                break
            else:
                logger.debug(f"âŒ Ð¡ÐµÐ»ÐµÐºÑ‚Ð¾Ñ€ Ñ†ÐµÐ½Ñ‹ '{selector}' Ð½Ðµ ÑÑ€Ð°Ð±Ð¾Ñ‚Ð°Ð»")
        
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ñ‡Ð¸ÑÐ»Ð¾Ð²ÑƒÑŽ Ñ†ÐµÐ½Ñƒ
        price_numeric = self._extract_price_debug(price_text)
        
        # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð½Ñ‹Ðµ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð´Ð»Ñ Ð»Ð¾ÐºÐ°Ñ†Ð¸Ð¸
        location_selectors = [
            '[data-testid="location-date"]',
            '.bottom-cell',
            '[data-cy="ad-card-location"]',
            '.css-1a4brun'
        ]
        
        location = "N/A"
        for selector in location_selectors:
            location_elem = item.select_one(selector)
            if location_elem:
                location = location_elem.get_text(strip=True)
                logger.info(f"âœ… Ð›Ð¾ÐºÐ°Ñ†Ð¸Ñ Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ñ‡ÐµÑ€ÐµÐ· '{selector}': '{location}'")
                break
            else:
                logger.debug(f"âŒ Ð¡ÐµÐ»ÐµÐºÑ‚Ð¾Ñ€ Ð»Ð¾ÐºÐ°Ñ†Ð¸Ð¸ '{selector}' Ð½Ðµ ÑÑ€Ð°Ð±Ð¾Ñ‚Ð°Ð»")
        
        # Ð˜Ñ‰ÐµÐ¼ ÑÑÑ‹Ð»ÐºÑƒ
        link_elem = item.select_one('a')
        relative_link = link_elem.get('href', '') if link_elem else ''
        full_link = urljoin(self.base_url, relative_link) if relative_link else 'N/A'
        
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒ
        area = self._extract_area_debug(title)
        
        result = {
            'title': title,
            'price_text': price_text,
            'price_numeric': price_numeric,
            'area': area,
            'location': location,
            'link': full_link
        }
        
        logger.info(f"ðŸ“Š Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ: {result}")
        return result

    def _extract_price_debug(self, price_text: str) -> Optional[float]:
        """ÐžÑ‚Ð»Ð°Ð´Ð¾Ñ‡Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ñ†ÐµÐ½Ñ‹"""
        logger.info(f"ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ†ÐµÐ½Ñƒ: '{price_text}'")
        
        if not price_text or price_text == 'N/A':
            logger.info("Ð¦ÐµÐ½Ð° Ð¿ÑƒÑÑ‚Ð°Ñ Ð¸Ð»Ð¸ N/A")
            return None
        
        # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð²ÑÐµ ÐºÑ€Ð¾Ð¼Ðµ Ñ†Ð¸Ñ„Ñ€ Ð¸ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð¾Ð²
        price_clean = re.sub(r'[^\d\s]', '', price_text)
        price_clean = re.sub(r'\s+', '', price_clean)
        
        logger.info(f"ÐžÑ‡Ð¸Ñ‰ÐµÐ½Ð½Ð°Ñ Ñ†ÐµÐ½Ð°: '{price_clean}'")
        
        try:
            result = float(price_clean) if price_clean else None
            logger.info(f"Ð§Ð¸ÑÐ»Ð¾Ð²Ð°Ñ Ñ†ÐµÐ½Ð°: {result}")
            return result
        except ValueError as e:
            logger.info(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ð¸ Ñ†ÐµÐ½Ñ‹: {e}")
            return None

    def _extract_area_debug(self, title: str) -> Optional[float]:
        """ÐžÑ‚Ð»Ð°Ð´Ð¾Ñ‡Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¿Ð»Ð¾Ñ‰Ð°Ð´Ð¸"""
        logger.info(f"Ð˜Ñ‰ÐµÐ¼ Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒ Ð² Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐµ: '{title}'")
        
        # Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¿Ð»Ð¾Ñ‰Ð°Ð´Ð¸
        area_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*Ð¼Â²',        # 50 Ð¼Â², 75.5Ð¼Â²
            r'(\d+(?:[.,]\d+)?)\s*ÐºÐ²\.?Ð¼',     # 50 ÐºÐ².Ð¼, 75 ÐºÐ² Ð¼
            r'(\d+(?:[.,]\d+)?)\s*mÂ²',         # 50 mÂ²
            r'(\d+(?:[.,]\d+)?)\s*ÐºÐ²\.?\s*Ð¼',  # 50 ÐºÐ² Ð¼
        ]
        
        for pattern in area_patterns:
            match = re.search(pattern, title, re.IGNORECASE)
            if match:
                try:
                    area_str = match.group(1).replace(',', '.')
                    area = float(area_str)
                    logger.info(f"âœ… ÐŸÐ»Ð¾Ñ‰Ð°Ð´ÑŒ Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ð¿Ð¾ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñƒ '{pattern}': {area} Ð¼Â²")
                    return area
                except ValueError:
                    continue
        
        logger.info("âŒ ÐŸÐ»Ð¾Ñ‰Ð°Ð´ÑŒ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
        return None

    def analyze_data_debug(self, listings: List[Dict]):
        """ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¾Ð±Ñ€Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ"""
        logger.info(f"\n=== ÐÐÐÐ›Ð˜Ð— Ð¡ÐžÐ‘Ð ÐÐÐÐ«Ð¥ Ð”ÐÐÐÐ«Ð¥ ===")
        logger.info(f"Ð’ÑÐµÐ³Ð¾ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹: {len(listings)}")
        
        if not listings:
            logger.warning("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°")
            return
        
        # ÐÐ½Ð°Ð»Ð¸Ð· Ñ†ÐµÐ½
        prices = [l['price_numeric'] for l in listings if l['price_numeric'] is not None]
        logger.info(f"ÐžÐ±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñ Ñ†ÐµÐ½Ð¾Ð¹: {len(prices)}")
        if prices:
            logger.info(f"ÐœÐ¸Ð½. Ñ†ÐµÐ½Ð°: {min(prices):,} Ñ‚ÐµÐ½Ð³Ðµ")
            logger.info(f"ÐœÐ°ÐºÑ. Ñ†ÐµÐ½Ð°: {max(prices):,} Ñ‚ÐµÐ½Ð³Ðµ")
            logger.info(f"Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ñ†ÐµÐ½Ð°: {sum(prices)/len(prices):,.0f} Ñ‚ÐµÐ½Ð³Ðµ")
        
        # ÐÐ½Ð°Ð»Ð¸Ð· Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÐµÐ¹
        areas = [l['area'] for l in listings if l['area'] is not None]
        logger.info(f"ÐžÐ±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñ Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒÑŽ: {len(areas)}")
        if areas:
            logger.info(f"ÐœÐ¸Ð½. Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒ: {min(areas)} Ð¼Â²")
            logger.info(f"ÐœÐ°ÐºÑ. Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒ: {max(areas)} Ð¼Â²")
            logger.info(f"Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒ: {sum(areas)/len(areas):.1f} Ð¼Â²")
        
        # ÐÐ½Ð°Ð»Ð¸Ð· Ð»Ð¾ÐºÐ°Ñ†Ð¸Ð¹
        locations = [l['location'] for l in listings if l['location'] != 'N/A']
        unique_locations = list(set(locations))
        logger.info(f"Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð»Ð¾ÐºÐ°Ñ†Ð¸Ð¹: {len(unique_locations)}")
        for loc in unique_locations[:5]:
            logger.info(f"  â€¢ {loc}")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹
        logger.info(f"\n=== ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ Ð¤Ð˜Ð›Ð¬Ð¢Ð ÐžÐ’ ===")
        
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ð¿Ð»Ð¾Ñ‰Ð°Ð´Ð¸ (>= 50)
        area_filter_pass = len([l for l in listings if l['area'] and l['area'] >= 50])
        logger.info(f"ÐŸÑ€Ð¾ÑˆÐ»Ð¸ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð»Ð¾Ñ‰Ð°Ð´Ð¸ (>= 50 Ð¼Â²): {area_filter_pass}")
        
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ñ†ÐµÐ½Ðµ (<= 500000)
        price_filter_pass = len([l for l in listings if l['price_numeric'] and l['price_numeric'] <= 500000])
        logger.info(f"ÐŸÑ€Ð¾ÑˆÐ»Ð¸ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ Ñ†ÐµÐ½Ñ‹ (<= 500,000): {price_filter_pass}")
        
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ð»Ð¾ÐºÐ°Ñ†Ð¸Ð¸
        location_filter_pass = len([l for l in listings if any(keyword.lower() in l['location'].lower() 
                                                             for keyword in ['Ð°Ð»Ð¼Ð°Ñ‚Ñ‹', 'Ñ†ÐµÐ½Ñ‚Ñ€'])])
        logger.info(f"ÐŸÑ€Ð¾ÑˆÐ»Ð¸ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ Ð»Ð¾ÐºÐ°Ñ†Ð¸Ð¸ (ÐÐ»Ð¼Ð°Ñ‚Ñ‹/Ñ†ÐµÐ½Ñ‚Ñ€): {location_filter_pass}")
        
        # Ð’ÑÐµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹ Ð²Ð¼ÐµÑÑ‚Ðµ
        combined_pass = 0
        for l in listings:
            area_ok = l['area'] and l['area'] >= 50
            price_ok = l['price_numeric'] and l['price_numeric'] <= 500000
            location_ok = any(keyword.lower() in l['location'].lower() for keyword in ['Ð°Ð»Ð¼Ð°Ñ‚Ñ‹', 'Ñ†ÐµÐ½Ñ‚Ñ€'])
            
            if area_ok and price_ok and location_ok:
                combined_pass += 1
        
        logger.info(f"ÐŸÑ€Ð¾ÑˆÐ»Ð¸ Ð²ÑÐµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾: {combined_pass}")

    def run_debug(self):
        """Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ð¾Ñ‚Ð»Ð°Ð´Ð¾Ñ‡Ð½ÑƒÑŽ Ð²ÐµÑ€ÑÐ¸ÑŽ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°"""
        logger.info("ðŸ” Ð—ÐÐŸÐ£Ð¡Ðš ÐžÐ¢Ð›ÐÐ”ÐžÐ§ÐÐžÐ“Ðž Ð Ð•Ð–Ð˜ÐœÐ")
        self.setup_session()
        
        all_listings = []
        
        for page in range(1, self.max_pages + 1):
            page_url = f"{self.search_url}?page={page}"
            logger.info(f"\nðŸ“„ ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ {page}: {page_url}")
            
            listings = self.scrape_page_debug(page_url)
            all_listings.extend(listings)
            
            sleep(random.uniform(2, 4))
        
        # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¾Ð±Ñ€Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        self.analyze_data_debug(all_listings)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
        if all_listings:
            df = pd.DataFrame(all_listings)
            df.to_csv("debug_results.csv", index=False, encoding='utf-8-sig')
            logger.info("ÐžÑ‚Ð»Ð°Ð´Ð¾Ñ‡Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² debug_results.csv")


def main():
    parser = OLXParserDebug()
    parser.run_debug()


if __name__ == "__main__":
    main()