import requests
from bs4 import BeautifulSoup
import pandas as pd
from time import sleep
import random
import json
import logging
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin, urlparse
import re
from typing import List, Dict, Optional

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OLXParserDebug:
    def __init__(self):
        self.base_url = "https://www.olx.kz"
        self.search_url = "https://www.olx.kz/nedvizhimost/kommercheskie-pomeshcheniya/arenda/"
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        self.session = requests.Session()
        self.max_pages = 3  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        self.request_timeout = 10
        self.output_file = "olx_commercial_debug.csv"
        
    def setup_session(self) -> None:
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å–µ—Å—Å–∏—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏"""
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })

    def scrape_page_debug(self, page_url: str) -> List[Dict]:
        """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            self.session.headers['User-Agent'] = random.choice(self.user_agents)
            response = self.session.get(page_url, timeout=self.request_timeout)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò—â–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            items = soup.select('div[data-cy="l-card"]')
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ div[data-cy='l-card']")
            
            if not items:
                # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –∏ –≤—ã–≤–æ–¥–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                logger.warning("–û—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
                self._debug_page_structure(soup)
                return []
            
            listings = []
            
            for i, item in enumerate(items[:5]):  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                logger.info(f"\n--- –û—Ç–ª–∞–¥–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è #{i+1} ---")
                listing_data = self._extract_listing_data_debug(item)
                if listing_data:
                    listings.append(listing_data)
                    
            return listings
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            return []

    def _debug_page_structure(self, soup: BeautifulSoup):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
        logger.info("=== –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –°–¢–†–ê–ù–ò–¶–´ ===")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        selectors_to_check = [
            'div[data-cy="l-card"]',
            '.css-1sw7q4x',
            '[data-testid="l-card"]',
            '.offer-wrapper',
            'div[data-marker="item"]',
            'article',
            '.ad-card'
        ]
        
        for selector in selectors_to_check:
            elements = soup.select(selector)
            logger.info(f"–°–µ–ª–µ–∫—Ç–æ—Ä '{selector}': –Ω–∞–π–¥–µ–Ω–æ {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            
            if elements and len(elements) > 0:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                first_elem = elements[0]
                logger.info(f"–ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç '{selector}':")
                logger.info(f"  - –¢–µ–≥: {first_elem.name}")
                logger.info(f"  - –ö–ª–∞—Å—Å—ã: {first_elem.get('class', [])}")
                logger.info(f"  - –ê—Ç—Ä–∏–±—É—Ç—ã: {list(first_elem.attrs.keys())}")
                
                # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤–Ω—É—Ç—Ä–∏
                titles = first_elem.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                logger.info(f"  - –ù–∞–π–¥–µ–Ω–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {len(titles)}")
                for title in titles[:2]:
                    logger.info(f"    ‚Ä¢ {title.name}: '{title.get_text(strip=True)[:50]}...'")

    def _extract_listing_data_debug(self, item) -> Optional[Dict]:
        """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —ç–ª–µ–º–µ–Ω—Ç–∞: {item.name}, –∫–ª–∞—Å—Å—ã: {item.get('class', [])}")
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_selectors = [
            'h6', 'h4', 'h3', 'h2', 'h1',
            '[data-cy="ad-card-title"]', 
            'a[data-cy="listing-ad-title"]',
            '.css-16v5mdi',
            '.css-u2ayx9'
        ]
        
        title = "N/A"
        for selector in title_selectors:
            title_elem = item.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                logger.info(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ '{selector}': '{title[:50]}...'")
                break
            else:
                logger.debug(f"‚ùå –°–µ–ª–µ–∫—Ç–æ—Ä '{selector}' –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª")
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ü–µ–Ω—ã
        price_selectors = [
            '[data-testid="ad-price"]',
            '.price',
            '[data-cy="ad-card-price"]',
            '.css-10b0gli',
            '.css-1uwte2c'
        ]
        
        price_text = "N/A"
        for selector in price_selectors:
            price_elem = item.select_one(selector)
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                logger.info(f"‚úÖ –¶–µ–Ω–∞ –Ω–∞–π–¥–µ–Ω–∞ —á–µ—Ä–µ–∑ '{selector}': '{price_text}'")
                break
            else:
                logger.debug(f"‚ùå –°–µ–ª–µ–∫—Ç–æ—Ä —Ü–µ–Ω—ã '{selector}' –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤—É—é —Ü–µ–Ω—É
        price_numeric = self._extract_price_debug(price_text)
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ª–æ–∫–∞—Ü–∏–∏
        location_selectors = [
            '[data-testid="location-date"]',
            '.bottom-cell',
            '[data-cy="ad-card-location"]',
            '.css-1a4brun'
        ]
        
        location = "N/A"
        for selector in location_selectors:
            location_elem = item.select_one(selector)
            if location_elem:
                location = location_elem.get_text(strip=True)
                logger.info(f"‚úÖ –õ–æ–∫–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–∞ —á–µ—Ä–µ–∑ '{selector}': '{location}'")
                break
            else:
                logger.debug(f"‚ùå –°–µ–ª–µ–∫—Ç–æ—Ä –ª–æ–∫–∞—Ü–∏–∏ '{selector}' –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª")
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫—É
        link_elem = item.select_one('a')
        relative_link = link_elem.get('href', '') if link_elem else ''
        full_link = urljoin(self.base_url, relative_link) if relative_link else 'N/A'
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–ª–æ—â–∞–¥—å
        area = self._extract_area_debug(title)
        
        result = {
            'title': title,
            'price_text': price_text,
            'price_numeric': price_numeric,
            'area': area,
            'location': location,
            'link': full_link
        }
        
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {result}")
        return result

    def _extract_price_debug(self, price_text: str) -> Optional[float]:
        """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ü–µ–Ω—ã"""
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–Ω—É: '{price_text}'")
        
        if not price_text or price_text == 'N/A':
            logger.info("–¶–µ–Ω–∞ –ø—É—Å—Ç–∞—è –∏–ª–∏ N/A")
            return None
        
        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä –∏ –ø—Ä–æ–±–µ–ª–æ–≤
        price_clean = re.sub(r'[^\d\s]', '', price_text)
        price_clean = re.sub(r'\s+', '', price_clean)
        
        logger.info(f"–û—á–∏—â–µ–Ω–Ω–∞—è —Ü–µ–Ω–∞: '{price_clean}'")
        
        try:
            result = float(price_clean) if price_clean else None
            logger.info(f"–ß–∏—Å–ª–æ–≤–∞—è —Ü–µ–Ω–∞: {result}")
            return result
        except ValueError as e:
            logger.info(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ü–µ–Ω—ã: {e}")
            return None

    def _extract_area_debug(self, title: str) -> Optional[float]:
        """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–ª–æ—â–∞–¥–∏"""
        logger.info(f"–ò—â–µ–º –ø–ª–æ—â–∞–¥—å –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ: '{title}'")
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–ª–æ—â–∞–¥–∏
        area_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*–º¬≤',        # 50 –º¬≤, 75.5–º¬≤
            r'(\d+(?:[.,]\d+)?)\s*–∫–≤\.?–º',     # 50 –∫–≤.–º, 75 –∫–≤ –º
            r'(\d+(?:[.,]\d+)?)\s*m¬≤',         # 50 m¬≤
            r'(\d+(?:[.,]\d+)?)\s*–∫–≤\.?\s*–º',  # 50 –∫–≤ –º
        ]
        
        for pattern in area_patterns:
            match = re.search(pattern, title, re.IGNORECASE)
            if match:
                try:
                    area_str = match.group(1).replace(',', '.')
                    area = float(area_str)
                    logger.info(f"‚úÖ –ü–ª–æ—â–∞–¥—å –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É '{pattern}': {area} –º¬≤")
                    return area
                except ValueError:
                    continue
        
        logger.info("‚ùå –ü–ª–æ—â–∞–¥—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return None

    def analyze_data_debug(self, listings: List[Dict]):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        logger.info(f"\n=== –ê–ù–ê–õ–ò–ó –°–û–ë–†–ê–ù–ù–´–• –î–ê–ù–ù–´–• ===")
        logger.info(f"–í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
        
        if not listings:
            logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return
        
        # –ê–Ω–∞–ª–∏–∑ —Ü–µ–Ω
        prices = [l['price_numeric'] for l in listings if l['price_numeric'] is not None]
        logger.info(f"–û–±—ä—è–≤–ª–µ–Ω–∏–π —Å —Ü–µ–Ω–æ–π: {len(prices)}")
        if prices:
            logger.info(f"–ú–∏–Ω. —Ü–µ–Ω–∞: {min(prices):,} —Ç–µ–Ω–≥–µ")
            logger.info(f"–ú–∞–∫—Å. —Ü–µ–Ω–∞: {max(prices):,} —Ç–µ–Ω–≥–µ")
            logger.info(f"–°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞: {sum(prices)/len(prices):,.0f} —Ç–µ–Ω–≥–µ")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–ª–æ—â–∞–¥–µ–π
        areas = [l['area'] for l in listings if l['area'] is not None]
        logger.info(f"–û–±—ä—è–≤–ª–µ–Ω–∏–π —Å –ø–ª–æ—â–∞–¥—å—é: {len(areas)}")
        if areas:
            logger.info(f"–ú–∏–Ω. –ø–ª–æ—â–∞–¥—å: {min(areas)} –º¬≤")
            logger.info(f"–ú–∞–∫—Å. –ø–ª–æ—â–∞–¥—å: {max(areas)} –º¬≤")
            logger.info(f"–°—Ä–µ–¥–Ω—è—è –ø–ª–æ—â–∞–¥—å: {sum(areas)/len(areas):.1f} –º¬≤")
        
        # –ê–Ω–∞–ª–∏–∑ –ª–æ–∫–∞—Ü–∏–π
        locations = [l['location'] for l in listings if l['location'] != 'N/A']
        unique_locations = list(set(locations))
        logger.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ–∫–∞—Ü–∏–π: {len(unique_locations)}")
        for loc in unique_locations[:5]:
            logger.info(f"  ‚Ä¢ {loc}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
        logger.info(f"\n=== –ü–†–û–í–ï–†–ö–ê –§–ò–õ–¨–¢–†–û–í ===")
        
        # –§–∏–ª—å—Ç—Ä –ø–æ –ø–ª–æ—â–∞–¥–∏ (>= 50)
        area_filter_pass = len([l for l in listings if l['area'] and l['area'] >= 50])
        logger.info(f"–ü—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä –ø–ª–æ—â–∞–¥–∏ (>= 50 –º¬≤): {area_filter_pass}")
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ü–µ–Ω–µ (<= 500000)
        price_filter_pass = len([l for l in listings if l['price_numeric'] and l['price_numeric'] <= 500000])
        logger.info(f"–ü—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä —Ü–µ–Ω—ã (<= 500,000): {price_filter_pass}")
        
        # –§–∏–ª—å—Ç—Ä –ø–æ –ª–æ–∫–∞—Ü–∏–∏
        location_filter_pass = len([l for l in listings if any(keyword.lower() in l['location'].lower() 
                                                             for keyword in ['–∞–ª–º–∞—Ç—ã', '—Ü–µ–Ω—Ç—Ä'])])
        logger.info(f"–ü—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä –ª–æ–∫–∞—Ü–∏–∏ (–ê–ª–º–∞—Ç—ã/—Ü–µ–Ω—Ç—Ä): {location_filter_pass}")
        
        # –í—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã –≤–º–µ—Å—Ç–µ
        combined_pass = 0
        for l in listings:
            area_ok = l['area'] and l['area'] >= 50
            price_ok = l['price_numeric'] and l['price_numeric'] <= 500000
            location_ok = any(keyword.lower() in l['location'].lower() for keyword in ['–∞–ª–º–∞—Ç—ã', '—Ü–µ–Ω—Ç—Ä'])
            
            if area_ok and price_ok and location_ok:
                combined_pass += 1
        
        logger.info(f"–ü—Ä–æ—à–ª–∏ –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ: {combined_pass}")

    def run_debug(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –≤–µ—Ä—Å–∏—é –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        logger.info("üîç –ó–ê–ü–£–°–ö –û–¢–õ–ê–î–û–ß–ù–û–ì–û –†–ï–ñ–ò–ú–ê")
        self.setup_session()
        
        all_listings = []
        
        for page in range(1, self.max_pages + 1):
            page_url = f"{self.search_url}?page={page}"
            logger.info(f"\nüìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {page_url}")
            
            listings = self.scrape_page_debug(page_url)
            all_listings.extend(listings)
            
            sleep(random.uniform(2, 4))
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.analyze_data_debug(all_listings)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        if all_listings:
            df = pd.DataFrame(all_listings)
            df.to_csv("debug_results.csv", index=False, encoding='utf-8-sig')
            logger.info("–û—Ç–ª–∞–¥–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ debug_results.csv")


def main():
    parser = OLXParserDebug()
    parser.run_debug()


if __name__ == "__main__":
    main()